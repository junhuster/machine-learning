{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 16:48:45.497672: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-06 16:48:45.560580: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-06 16:48:47.054497: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth enabled for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'): True\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "sys.path.append('../../pytools')\n",
    "import d2l\n",
    "import os\n",
    "\n",
    "d2l.gpu_mem_init()\n",
    "tf.keras.config.disable_traceback_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class PositionWiseFFN(tf.keras.layers.Layer):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        super().__init__(*kwargs)\n",
    "        self.dense1 = tf.keras.layers.Dense(ffn_num_hiddens)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dense2 = tf.keras.layers.Dense(ffn_num_outputs)\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1770367728.229402 3401145 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8174 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:08.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(4, 8)\n",
    "res = ffn(tf.ones((2, 3, 4)))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm: tf.Tensor(\n",
      "[[-0.998006    0.9980061 ]\n",
      " [-0.9980061   0.99800587]], shape=(2, 2), dtype=float32) \n",
      "batch norm: tf.Tensor(\n",
      "[[-0.998006   -0.9980061 ]\n",
      " [ 0.9980061   0.99800587]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ln = tf.keras.layers.LayerNormalization()\n",
    "bn = tf.keras.layers.BatchNormalization()\n",
    "X = tf.constant([[1, 2], [2, 3]], dtype=tf.float32)\n",
    "print('layer norm:', ln(X), '\\nbatch norm:', bn(X, training=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class AddNorm(tf.keras.layers.Layer):\n",
    "    \"\"\"残差连接后进行层规范化\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.ln = tf.keras.layers.LayerNormalization(normalized_shape)\n",
    "        #print(f\"nshape:{normalized_shape}\")\n",
    "\n",
    "    def call(self, X, Y, **kwargs):\n",
    "        return self.ln(self.dropout(Y, **kwargs) + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:[[[ 2.140553   -1.0183516  -0.23402022  0.52439636]\n",
      "  [ 0.00798855 -2.2173953   0.04611805 -0.00617931]\n",
      "  [ 0.67126876  0.82275385  1.0642483   1.4899709 ]]\n",
      "\n",
      " [[-0.43353182 -0.2869015  -0.10166499  1.1965916 ]\n",
      "  [-0.15523644 -1.0718852  -1.4636456   0.78820497]\n",
      "  [ 1.6239073  -0.8156688   0.2225158   0.144486  ]]]\n",
      "m:0.27427926659584045, std:1.0971648693084717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[ 1.70082   , -1.1780331 , -0.4632364 ,  0.22794308],\n",
       "        [-0.24268283, -2.270776  , -0.20793368, -0.25559464],\n",
       "        [ 0.36179453,  0.4998498 ,  0.7199347 ,  1.107915  ]],\n",
       "\n",
       "       [[-0.46627563, -0.2970968 , -0.08337498,  1.414525  ],\n",
       "        [-0.14518455, -1.2027938 , -1.6547983 ,  0.9433375 ],\n",
       "        [ 1.9075525 , -0.9071769 ,  0.2906577 ,  0.20062862]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_norm = AddNorm([1, 2], 0.5)\n",
    "x = tf.random.normal((2,3,4))\n",
    "print(f\"x:{x}\")\n",
    "mean = np.mean(x[0])\n",
    "std = np.std(x[0])\n",
    "print(f\"m:{mean}, std:{std}\")\n",
    "add_norm(x, x, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Transformer编码器块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens,\n",
    "                                                num_heads, dropout, bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def call(self, X, valid_lens, **kwargs):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens, **kwargs), **kwargs)\n",
    "        return self.addnorm2(Y, self.ffn(Y), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:1484: UserWarning: Layer 'multi_head_attention' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''got an unexpected keyword argument 'kwargs'''\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'multi_head_attention', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 100, 24])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.ones((2, 100, 24))\n",
    "valid_lens = tf.constant([3, 2])\n",
    "norm_shape = [i for i in range(len(X.shape))][1:]\n",
    "encoder_blk = EncoderBlock(24, 24, 24, 24, norm_shape, 48, 8, 0.5)\n",
    "encoder_blk(X, valid_lens, training=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class TransformerEncoder(d2l.Encoder):\n",
    "    \"\"\"Transformer编码器\"\"\"\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
    "                 num_hiddens, norm_shape, ffn_num_hiddens, num_heads,\n",
    "                 num_layers, dropout, bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = [EncoderBlock(\n",
    "            key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "            ffn_num_hiddens, num_heads, dropout, bias) for _ in range(\n",
    "            num_layers)]\n",
    "\n",
    "    def call(self, X, valid_lens, **kwargs):\n",
    "        # 因为位置编码值在-1和1之间，\n",
    "        # 因此嵌入值乘以嵌入维度的平方根进行缩放，\n",
    "        # 然后再与位置编码相加。\n",
    "        X = self.pos_encoding(self.embedding(X) * tf.math.sqrt(\n",
    "            tf.cast(self.num_hiddens, dtype=tf.float32)), **kwargs)\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens, **kwargs)\n",
    "            self.attention_weights[\n",
    "                i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:1484: UserWarning: Layer 'multi_head_attention_1' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''got an unexpected keyword argument 'kwargs'''\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'multi_head_attention_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:1484: UserWarning: Layer 'multi_head_attention_2' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''got an unexpected keyword argument 'kwargs'''\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'multi_head_attention_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 100, 24])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(200, 24, 24, 24, 24, [1, 2], 48, 8, 2, 0.5)\n",
    "encoder(tf.ones((2, 100)), valid_lens, training=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"解码器中第i个块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_hiddens, num_heads, dropout, i, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.i = i\n",
    "        self.attention1 = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.attention2 = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    ## TODO 原代码def call() 会报错，改成__call()__不报错, 原因待查\n",
    "    def __call__(self, X, state, **kwargs):\n",
    "        \n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        # 训练阶段，输出序列的所有词元都在同一时间处理，\n",
    "        # 因此state[2][self.i]初始化为None。\n",
    "        # 预测阶段，输出序列是通过词元一个接着一个解码的，\n",
    "        # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示\n",
    "        \n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = tf.concat((state[2][self.i], X), axis=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if kwargs['training']:\n",
    "            batch_size, num_steps, _ = X.shape\n",
    "           # dec_valid_lens的开头:(batch_size,num_steps),\n",
    "            # 其中每一行是[1,2,...,num_steps]\n",
    "            dec_valid_lens = tf.repeat(tf.reshape(tf.range(1, num_steps + 1),\n",
    "                                                 shape=(-1, num_steps)), repeats=batch_size, axis=0)\n",
    "\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "\n",
    "        # 自注意力\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens, **kwargs)\n",
    "        Y = self.addnorm1(X, X2, **kwargs)\n",
    "        # 编码器－解码器注意力。\n",
    "        # enc_outputs的开头:(batch_size,num_steps,num_hiddens)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens, **kwargs)\n",
    "        Z = self.addnorm2(Y, Y2, **kwargs)\n",
    "        return self.addnorm3(Z, self.ffn(Z), **kwargs), state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 100, 24])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_blk = DecoderBlock(24, 24, 24, 24, [1, 2], 48, 8, 0.5, 0)\n",
    "X = tf.ones((2, 100, 24))\n",
    "state = [encoder_blk(X, valid_lens), valid_lens, [None]]\n",
    "decoder_blk(X, state, training=False)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(d2l.AttentionDecoder):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
    "                 num_hiddens, norm_shape, ffn_num_hidens, num_heads, num_layers, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = [DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                                  ffn_num_hiddens, num_heads, dropout, i) for i in range(num_layers)]\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n",
    "    ##TODO call-->__call()__\n",
    "    def __call__(self, X, state, **kwargs):\n",
    "        X = self.pos_encoding(self.embedding(X) * tf.math.sqrt(tf.cast(self.num_hiddens, dtype=tf.float32)), **kwargs)\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]  # 解码器中2个注意力层\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state, **kwargs)\n",
    "            # 解码器自注意力权重\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            # “编码器－解码器”自注意力权重\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.026, 1666.1 tokens/sec on <tensorflow.python.eager.context._EagerDeviceContext object at 0x7fd3b0193000>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_hiddens, num_layers, dropout, batch_size, num_steps =32, 1, 0.1, 128, 10\n",
    "lr, num_epochs, device = 0.005, 200, d2l.try_gpu()\n",
    "ffn_num_hiddens, num_heads = 64, 4\n",
    "key_size, query_size, value_size = 32, 32, 32\n",
    "norm_shape = [2]\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = TransformerEncoder(\n",
    "    len(src_vocab), key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "    ffn_num_hiddens, num_heads, num_layers, dropout)\n",
    "decoder = TransformerDecoder(\n",
    "    len(tgt_vocab), key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "    ffn_num_hiddens, num_heads, num_layers, dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va !,  bleu 1.000\n",
      "i lost . => j'ai perdu .,  bleu 1.000\n",
      "he's calm . => il est calme .,  bleu 1.000\n",
      "i'm home . => je suis chez moi .,  bleu 1.000\n"
     ]
    }
   ],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, dec_attention_weight_seq = d2l.predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, True)\n",
    "    print(f'{eng} => {translation}, ',\n",
    "          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
