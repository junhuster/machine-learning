{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth enabled for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'): True\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('../pytools')\n",
    "import d2l\n",
    "\n",
    "d2l.gpu_mem_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "mi_train,mi_test = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "w = tf.Variable(tf.random.normal(shape=(num_inputs, num_outputs), mean=0.0, stddev=0.01))\n",
    "b = tf.Variable(tf.zeros(shape=(num_outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = tf.exp(x)\n",
    "    partiton = tf.reduce_sum(x_exp, axis=1, keepdims=True)\n",
    "    return x_exp/partiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(x):\n",
    "    return softmax(tf.matmul(tf.reshape(x, (-1, w.shape[0])), w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return -tf.math.log(tf.boolean_mask(y_hat, tf.one_hot(y, depth=(y_hat.shape[-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = tf.argmax(y_hat, axis=1)\n",
    "    cmp = tf.cast(y_hat, dtype=y.dtype) == y\n",
    "    return float(tf.reduce_sum(tf.cast(cmp, y.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1695"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_accuracy(net, data_iter):\n",
    "    metric = Accumulator(2)\n",
    "    for x,y in data_iter:\n",
    "        metric.add(accuracy(net(x), y), len(y))\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "evaluate_accuracy(net, mi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1695"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(net, mi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):  #@save\n",
    "    \"\"\"训练模型一个迭代周期（定义见第3章）\"\"\"\n",
    "    # 训练损失总和、训练准确度总和、样本数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # 计算梯度并更新参数\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = net(X)\n",
    "            # Keras内置的损失接受的是（标签，预测），这不同于用户在本书中的实现。\n",
    "            # 本书的实现接受（预测，标签），例如我们上面实现的“交叉熵”\n",
    "            if isinstance(loss, tf.keras.losses.Loss):\n",
    "                l = loss(y, y_hat)\n",
    "            else:\n",
    "                l = loss(y_hat, y)\n",
    "        if isinstance(updater, tf.keras.optimizers.Optimizer):\n",
    "            params = net.trainable_variables\n",
    "            grads = tape.gradient(l, params)\n",
    "            updater.apply_gradients(zip(grads, params))\n",
    "        else:\n",
    "            updater(X.shape[0], tape.gradient(l, updater.params))\n",
    "        # Keras的loss默认返回一个批量的平均损失\n",
    "        l_sum = l * float(tf.size(y)) if isinstance(\n",
    "            loss, tf.keras.losses.Loss) else tf.reduce_sum(l)\n",
    "        metric.add(l_sum, accuracy(y_hat, y), tf.size(y))\n",
    "    # 返回训练损失和训练精度\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        train_loss, train_acc = train_metrics\n",
    "        print(f'epoch:{epoch}, train_loss:{train_loss:f}, train_auc:{train_acc:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Updater():  #@save\n",
    "    \"\"\"用小批量随机梯度下降法更新参数\"\"\"\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def __call__(self, batch_size, grads):\n",
    "        d2l.sgd(self.params, grads, self.lr, batch_size)\n",
    "\n",
    "updater = Updater([w, b], lr=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:0.820260, train_auc:0.743317\n",
      "epoch:1, train_loss:0.589211, train_auc:0.809633\n",
      "epoch:2, train_loss:0.539995, train_auc:0.822900\n",
      "epoch:3, train_loss:0.514178, train_auc:0.829317\n",
      "epoch:4, train_loss:0.496863, train_auc:0.834300\n",
      "epoch:5, train_loss:0.484173, train_auc:0.838183\n",
      "epoch:6, train_loss:0.474604, train_auc:0.840750\n",
      "epoch:7, train_loss:0.467193, train_auc:0.843417\n",
      "epoch:8, train_loss:0.460513, train_auc:0.844617\n",
      "epoch:9, train_loss:0.455114, train_auc:0.846817\n",
      "epoch:10, train_loss:0.451351, train_auc:0.848200\n",
      "epoch:11, train_loss:0.446912, train_auc:0.850233\n",
      "epoch:12, train_loss:0.442549, train_auc:0.849467\n",
      "epoch:13, train_loss:0.440297, train_auc:0.850600\n",
      "epoch:14, train_loss:0.436671, train_auc:0.851450\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "train_ch3(net, mi_train, mi_test, cross_entropy, num_epochs, updater)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
