{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 15:53:34.081873: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-19 15:53:34.143625: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-19 15:53:35.616036: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth enabled for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'): True\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "sys.path.append('../pytools')\n",
    "import d2l\n",
    "\n",
    "d2l.gpu_mem_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758268416.595800 3575871 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30495 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0\n",
      "2025-09-19 15:53:36.841060: W external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:237] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 7.0\n",
      "2025-09-19 15:53:36.841095: W external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:240] Used ptxas at /usr/local/cuda/bin/ptxas\n",
      "2025-09-19 15:53:36.841171: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.844768: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.846741: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.849006: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.850906: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.852833: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.855135: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.857279: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.859497: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.863724: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.872461: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.875823: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:36.877758: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:53:37.338478: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[0.],\n",
       "       [0.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(name='flatten0'),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu, name='Dense0'),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu, name='Dense1'),\n",
    "    tf.keras.layers.Dense(1,name='Dense2'),\n",
    "], name='Out')\n",
    "\n",
    "X = tf.random.uniform((2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Flatten name=flatten0, built=True>\n",
      "[<Variable path=Out/Dense0/kernel, shape=(4, 4), dtype=float32, value=[[ 0.23205775  0.31370646  0.55850834  0.15237123]\n",
      " [-0.2567699  -0.2192471   0.38721746  0.7242524 ]\n",
      " [ 0.14606684  0.6064181   0.3240494   0.19513768]\n",
      " [ 0.04005933 -0.59194636 -0.14155227 -0.44525963]]>, <Variable path=Out/Dense0/bias, shape=(4,), dtype=float32, value=[0. 0. 0. 0.]>]\n",
      "[<Variable path=Out/Dense1/kernel, shape=(4, 4), dtype=float32, value=[[ 0.37543422  0.13030589 -0.71500844 -0.8276961 ]\n",
      " [ 0.06477106 -0.09510988 -0.1312887   0.15442067]\n",
      " [-0.21385974  0.18901962  0.15185004 -0.8493795 ]\n",
      " [-0.46930444 -0.81070864 -0.40376592 -0.417464  ]]>, <Variable path=Out/Dense1/bias, shape=(4,), dtype=float32, value=[0. 0. 0. 0.]>]\n",
      "[<Variable path=Out/Dense2/kernel, shape=(4, 1), dtype=float32, value=[[ 0.5050509 ]\n",
      " [-0.54444116]\n",
      " [ 0.57134926]\n",
      " [ 0.4959364 ]]>, <Variable path=Out/Dense2/bias, shape=(1,), dtype=float32, value=[0.]>]\n"
     ]
    }
   ],
   "source": [
    "print(net.layers[0])\n",
    "print(net.layers[1].weights)\n",
    "print(net.layers[2].weights)\n",
    "print(net.layers[3].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.src.backend.Variable'>\n",
      "<Variable path=Out/Dense0/bias, shape=(4,), dtype=float32, value=[0. 0. 0. 0.]>\n",
      "tf.Tensor(\n",
      "[[ 0.23205775  0.31370646  0.55850834  0.15237123]\n",
      " [-0.2567699  -0.2192471   0.38721746  0.7242524 ]\n",
      " [ 0.14606684  0.6064181   0.3240494   0.19513768]\n",
      " [ 0.04005933 -0.59194636 -0.14155227 -0.44525963]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(type(net.layers[1].weights[0]))\n",
    "print(net.layers[1].weights[1])\n",
    "print(tf.convert_to_tensor(net.layers[1].weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.37543422  0.13030589 -0.71500844 -0.8276961 ]\n",
      " [ 0.06477106 -0.09510988 -0.1312887   0.15442067]\n",
      " [-0.21385974  0.18901962  0.15185004 -0.8493795 ]\n",
      " [-0.46930444 -0.81070864 -0.40376592 -0.417464  ]]\n"
     ]
    }
   ],
   "source": [
    "print(net.get_weights()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[ 0.        ],\n",
       "       [-0.02992861]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1(name):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4, activation=tf.nn.relu)],\n",
    "        name=name)\n",
    "\n",
    "def block2():\n",
    "    net = tf.keras.Sequential(name='block2-out')\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add(block1(name=f'block2-in-{i}'))\n",
    "    return net\n",
    "\n",
    "rgnet = tf.keras.Sequential(name='Out')\n",
    "rgnet.add(block2())\n",
    "rgnet.add(tf.keras.layers.Dense(1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rgnet.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out/block2-out/block2-in-1/flatten_1\n"
     ]
    }
   ],
   "source": [
    "print(rgnet.layers[0].layers[1].layers[0].path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInit(tf.keras.initializers.Initializer):\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        data = tf.random.uniform(shape, -10, 10, dtype=dtype)\n",
    "        factor = (tf.abs(data) > 5)\n",
    "        factor = tf.cast(factor, dtype=tf.float32)\n",
    "        return data * factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[31.270807 ],\n",
       "       [10.2791195]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation='relu', kernel_initializer=MyInit),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "x = tf.random.uniform((2,20))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Variable path=sequential_15/dense_35/kernel, shape=(4, 4), dtype=float32, value=[[-0.8111685  -0.69305027 -0.6943098   0.45135564]\n",
      " [-0.16501623 -0.13279682  0.5357911   0.11565661]\n",
      " [-0.13937807 -0.71508384 -0.12512231  0.5382703 ]\n",
      " [ 0.5758949  -0.14786196  0.6496212   0.1637854 ]]>, <Variable path=sequential_15/dense_35/bias, shape=(4,), dtype=float32, value=[0. 0. 0. 0.]>]\n"
     ]
    }
   ],
   "source": [
    "# tf.keras的表现有点不同。它会自动删除重复层\n",
    "# 实际测试看tf2.20 & keras 3.11.3并不会删除重复层\n",
    "shared = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    shared,\n",
    "    shared,\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "# 检查参数是否不同\n",
    "#print(len(net.layers))\n",
    "#print(net.layers[1].weights)\n",
    "print(net.layers[2].weights)\n",
    "#print(net.layers[3].weights[0].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[net.layers[i].get_weights() for i in range(len(net.layers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 256), (256,), (256, 10), (10,)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((2,20))\n",
    "net(x)\n",
    "[w.shape for w in net.get_weights()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
