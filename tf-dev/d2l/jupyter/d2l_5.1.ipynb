{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 15:41:09.223088: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-19 15:41:09.286546: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-19 15:41:10.747886: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth enabled for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'): True\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "sys.path.append('../pytools')\n",
    "import d2l\n",
    "\n",
    "d2l.gpu_mem_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758267671.710976 3532719 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31135 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0\n",
      "2025-09-19 15:41:11.957574: W external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:237] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 7.0\n",
      "2025-09-19 15:41:11.957598: W external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:240] Used ptxas at /usr/local/cuda/bin/ptxas\n",
      "2025-09-19 15:41:11.957644: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.959743: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.962218: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.964155: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.966451: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.968562: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.970931: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.973199: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.975372: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.977516: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.988306: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.990575: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:11.992691: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-19 15:41:12.455420: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[ 0.06374773, -0.05352915,  0.24876606, -0.37581503,  0.02651278,\n",
       "         0.69608605, -0.52900094,  0.12065746,  0.19304925,  0.01788986],\n",
       "       [-0.14371297, -0.02770487,  0.13521667, -0.3029896 , -0.2068989 ,\n",
       "         0.48878407, -0.17565036,  0.32239825,  0.4242543 , -0.19951338]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "x = tf.random.uniform((2, 20, 2))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(10)\n",
    "    def call(self, x):\n",
    "        return self.out(self.hidden(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[ 1.01327404e-01, -2.08832428e-01,  1.81563497e-01,\n",
       "        -2.29557827e-01, -1.78273141e-01, -3.11898619e-01,\n",
       "         2.02904701e-01,  3.08429971e-02,  4.09111381e-05,\n",
       "         3.61326814e-01],\n",
       "       [-1.76479578e-01, -3.00132453e-01,  2.56714702e-01,\n",
       "        -2.30226427e-01, -3.88154909e-02, -3.57635140e-01,\n",
       "         2.33083576e-01,  5.71233407e-02,  1.01431563e-01,\n",
       "         2.02518225e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "x = tf.random.uniform((2, 16))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(tf.keras.Model):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.models = []\n",
    "        for blk in args:\n",
    "            self.models.append(blk)\n",
    "    def call(self, x):\n",
    "        for blk in self.models:\n",
    "            x = blk(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-0.09115406, -0.09465532, -0.07273813,  0.03880987,  0.3292024 ,\n",
       "        -0.12570827,  0.07184786, -0.05869579, -0.09463314,  0.29048485],\n",
       "       [ 0.01789289,  0.17741422,  0.14595613, -0.07918834,  0.37817797,\n",
       "        -0.08254399,  0.06946908, -0.06000722, -0.10639155,  0.17654687]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    ")\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond(x,y):\n",
    "    return tf.reduce_sum(tf.abs(x)) > 1\n",
    "def body(x,y):\n",
    "    out = y / 2\n",
    "    return out,out\n",
    "class FixedHiddenMlp(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.random_weight = tf.random.uniform((20,20))\n",
    "        self.dense = tf.keras.layers.Dense(20, activation='relu')\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = tf.nn.relu(tf.matmul(x, self.random_weight) + 1)\n",
    "        x = self.dense(x)\n",
    "        #原书中这个地方直接用python表达式，直接调用FixedHiddenMlp可以跑成功；但是在下文中作为tf.keras.Sequential的某一层\n",
    "        #运行会失败，改成tf.while_loop能运行成功\n",
    "        out = tf.while_loop(cond, body, [x,x])\n",
    "        return tf.reduce_sum(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.624274492263794>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMlp()\n",
    "x = tf.random.uniform((2,20))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestMLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential()\n",
    "        self.net.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.net.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        self.dense = tf.keras.layers.Dense(16, activation='relu')\n",
    "    def call(self, x):\n",
    "        return self.dense(self.net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixnet = tf.keras.Sequential()\n",
    "mixnet.add(NestMLP())\n",
    "mixnet.add(tf.keras.layers.Dense(20))\n",
    "mixnet.add(FixedHiddenMlp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.270679   0.69886506 0.23550093 0.30138934 0.5368024  0.78038657\n",
      "  0.9007256  0.18428016 0.764475   0.9831492  0.5891861  0.4123236\n",
      "  0.73847747 0.7795919  0.42276096 0.97720075 0.87284446 0.5005188\n",
      "  0.3752228  0.43402815]\n",
      " [0.74154425 0.45270002 0.040434   0.0415678  0.32297945 0.9199375\n",
      "  0.34642684 0.59795105 0.813846   0.4638996  0.40777338 0.944922\n",
      "  0.3030237  0.8540398  0.65797293 0.5134666  0.5534369  0.58092976\n",
      "  0.4765967  0.23661697]], shape=(2, 20), dtype=float32)\n",
      "tf.Tensor(1.5981015, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((2, 20))\n",
    "y = mixnet(x)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mixnet.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixnet.save_weights('mix.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.270679   0.69886506 0.23550093 0.30138934 0.5368024  0.78038657\n",
      "  0.9007256  0.18428016 0.764475   0.9831492  0.5891861  0.4123236\n",
      "  0.73847747 0.7795919  0.42276096 0.97720075 0.87284446 0.5005188\n",
      "  0.3752228  0.43402815]\n",
      " [0.74154425 0.45270002 0.040434   0.0415678  0.32297945 0.9199375\n",
      "  0.34642684 0.59795105 0.813846   0.4638996  0.40777338 0.944922\n",
      "  0.3030237  0.8540398  0.65797293 0.5134666  0.5534369  0.58092976\n",
      "  0.4765967  0.23661697]], shape=(2, 20), dtype=float32)\n",
      "tf.Tensor(1.4902631, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mixnet0 = tf.keras.Sequential()\n",
    "mixnet0.add(NestMLP())\n",
    "mixnet0.add(tf.keras.layers.Dense(20))\n",
    "mixnet0.add(FixedHiddenMlp())\n",
    "mixnet0(x)\n",
    "mixnet0.load_weights('mix.weights.h5')\n",
    "print(x)\n",
    "print(mixnet0(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
