{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造输入\n",
    "def construct_data(w, b, num_samples): #@save\n",
    "    x = tf.zeros((num_samples, w.shape[0]))\n",
    "    x += tf.random.normal(x.shape)\n",
    "    y = tf.matmul(x, tf.reshape(w, (-1, 1))) + b\n",
    "    y += tf.random.normal(y.shape, stddev=0.01)\n",
    "    y = tf.reshape(y, (-1, 1))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = tf.constant([2, -3.4])\n",
    "true_b = 5.6\n",
    "features,lables = construct_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#遍历输入\n",
    "def data_iter(batch_size, features, labels):\n",
    "    nums = len(features)\n",
    "    indices = list(tf.range(nums))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, nums, batch_size):\n",
    "        j = indices[i: min(i + batch_size, nums)]\n",
    "        yield tf.gather(features, j), tf.gather(lables, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化参数\n",
    "w = tf.Variable(tf.random.normal(shape=(2,1), mean=0, stddev=0.01), trainable=True)\n",
    "b = tf.Variable(tf.zeros(1), trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型\n",
    "def lingre(x, w, b):\n",
    "    return tf.matmul(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数\n",
    "def squaloss(y_hat, y):\n",
    "    return (y_hat - tf.reshape(y, y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义优化函数\n",
    "def sgd(params, grads, lr, batch_size):\n",
    "    for param,grad in zip(params, grads):\n",
    "        param.assign_sub(lr*grad/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoc 0, loss 0.006964\n",
      "epoc 1, loss 0.000959\n",
      "epoc 2, loss 0.000171\n",
      "epoc 3, loss 0.000069\n",
      "w=<tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[ 1.9985149],\n",
      "       [-3.3973668]], dtype=float32)> \n",
      "b=<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([5.595611], dtype=float32)> \n",
      "true_w=[ 2.  -3.4] \n",
      "true_b=5.6\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "num_epochs = 4\n",
    "lr = 0.01\n",
    "net = lingre\n",
    "loss = squaloss\n",
    "batch_size = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for x,y in data_iter(batch_size, features, lables):\n",
    "        with tf.GradientTape() as t:\n",
    "            l = loss(net(x, w, b), y)\n",
    "            grads = t.gradient(l, [w, b])\n",
    "            sgd([w, b], grads, lr, batch_size)\n",
    "    train_l = loss(net(features, w, b), lables)\n",
    "    print(f'epoc {epoch}, loss {float(tf.reduce_mean(train_l)):f}')\n",
    "print(f'w={w} \\nb={b} \\ntrue_w={true_w} \\ntrue_b={true_b}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
