{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 16:16:14.430135: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-07 16:16:14.492262: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-07 16:16:15.991571: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757232976.923413 4055161 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31135 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(tf.range(4, dtype=tf.float32))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0.],\n",
       "       [0., 1., 2., 3.],\n",
       "       [0., 2., 4., 6.],\n",
       "       [0., 3., 6., 9.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = tf.tensordot(x, x, axes=0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([12., 12., 12., 12.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = t.gradient(y, x)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n",
       "array([[[ 0.,  1.],\n",
       "        [ 2.,  3.],\n",
       "        [ 4.,  5.]],\n",
       "\n",
       "       [[ 6.,  7.],\n",
       "        [ 8.,  9.],\n",
       "        [10., 11.]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.reshape(tf.range(12, dtype=tf.float32), (2,3,2))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 4), dtype=float32, numpy=\n",
       "array([[[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.]],\n",
       "\n",
       "       [[ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.]],\n",
       "\n",
       "       [[16., 17., 18., 19.],\n",
       "        [20., 21., 22., 23.]]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.reshape(tf.range(24, dtype=tf.float32), (3,2,4))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[220., 235., 250., 265.],\n",
       "       [580., 631., 682., 733.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tf.tensordot(a, b, axes=2)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[220., 235., 250., 265.],\n",
       "       [580., 631., 682., 733.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va = tf.Variable(a)\n",
    "vb = tf.Variable(b)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    y = tf.tensordot(va, vb, axes=2)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n",
       "array([[[ 6., 22.],\n",
       "        [38., 54.],\n",
       "        [70., 86.]],\n",
       "\n",
       "       [[ 6., 22.],\n",
       "        [38., 54.],\n",
       "        [70., 86.]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = t.gradient(y, va)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 4), dtype=float32, numpy=\n",
       "array([[[ 6.,  6.,  6.,  6.],\n",
       "        [ 8.,  8.,  8.,  8.]],\n",
       "\n",
       "       [[10., 10., 10., 10.],\n",
       "        [12., 12., 12., 12.]],\n",
       "\n",
       "       [[14., 14., 14., 14.],\n",
       "        [16., 16., 16., 16.]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_b = t.gradient(y, vb)\n",
    "grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=\n",
       "array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "       0.16666667], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = tf.ones(6) / 6\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 16:16:18.247361: W external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:237] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 7.0\n",
      "2025-09-07 16:16:18.247383: W external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:240] Used ptxas at /usr/local/cuda/bin/ptxas\n",
      "2025-09-07 16:16:18.247425: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.391252: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.393348: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.395594: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.397689: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.399838: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.402256: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.404348: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.406475: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.717974: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.720394: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.722292: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.724456: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.726563: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.729253: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.731333: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:18.734052: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:19.055171: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:19.057153: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:19.059397: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:19.061492: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:19.063716: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:19.065727: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:19.068045: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:19.070112: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.005972: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.008125: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.011344: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.013464: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.015785: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.018004: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.020868: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.022707: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.025191: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.499182: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.506856: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.572316: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.574737: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.577042: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.579028: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.581467: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.583362: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.585960: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.587963: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.811168: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.866811: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.927015: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:20.979223: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.031796: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.155294: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.157613: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.159748: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.162492: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.164648: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.167142: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.169136: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.171483: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.389055: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfp.distributions.Multinomial(1, prob).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([0., 0., 2., 0., 2., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfp.distributions.Multinomial(5, prob).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float64, numpy=\n",
       "array([0.        , 0.16666667, 0.33333333, 0.5       , 0.66666667,\n",
       "       0.83333333])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = tf.range(6) / 6\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 16:16:21.554345: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 16:16:21.689736: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.691704: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.694554: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.696718: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.699111: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.702166: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.704315: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:21.706470: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.006261: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.008910: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.010404: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.012573: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.014823: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.016880: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.019064: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.021221: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.957679: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.960011: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.962611: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.964868: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.966973: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.969283: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.971960: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.973800: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:22.975869: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:23.447028: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:23.451740: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:23.460105: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:23.531449: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:23.583701: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:23.635693: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-09-07 16:16:23.689494: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:188] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float64, numpy=array([0., 0., 0., 0., 2., 1.])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfp.distributions.Multinomial(3, prob).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__builtins__', '__call__', '__class__', '__closure__', '__code__', '__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__globals__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__kwdefaults__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__original_wrapped__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__signature__', '__sizeof__', '__str__', '__subclasshook__', '__wrapped__', '_tf_decorator', '_tf_fallback_dispatchers', '_tf_type_based_dispatcher']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tf.reduce_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ones in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "ones(shape, dtype=tf.float32, name=None, layout=None)\n",
      "    Creates a tensor with all elements set to one (1).\n",
      "    \n",
      "    See also `tf.ones_like`, `tf.zeros`, `tf.fill`, `tf.eye`.\n",
      "    \n",
      "    This operation returns a tensor of type `dtype` with shape `shape` and\n",
      "    all elements set to one.\n",
      "    \n",
      "    >>> tf.ones([3, 4], tf.int32)\n",
      "    <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
      "    array([[1, 1, 1, 1],\n",
      "           [1, 1, 1, 1],\n",
      "           [1, 1, 1, 1]], dtype=int32)>\n",
      "    \n",
      "    Args:\n",
      "      shape: A `list` of integers, a `tuple` of integers, or a 1-D `Tensor` of\n",
      "        type `int32`.\n",
      "      dtype: Optional DType of an element in the resulting `Tensor`. Default is\n",
      "        `tf.float32`.\n",
      "      name: Optional string. A name for the operation.\n",
      "      layout: Optional, `tf.experimental.dtensor.Layout`. If provided, the result\n",
      "        is a [DTensor](https://www.tensorflow.org/guide/dtensor_overview) with the\n",
      "        provided layout.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` with all elements set to one (1).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function gradients_v2 in module tensorflow.python.ops.gradients_impl:\n",
      "\n",
      "gradients_v2(ys, xs, grad_ys=None, name='gradients', gate_gradients=False, aggregation_method=None, stop_gradients=None, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>)\n",
      "    Constructs symbolic derivatives of sum of `ys` w.r.t. x in `xs`.\n",
      "    \n",
      "    `tf.gradients` is only valid in a graph context. In particular,\n",
      "    it is valid in the context of a `tf.function` wrapper, where code\n",
      "    is executing as a graph.\n",
      "    \n",
      "    `ys` and `xs` are each a `Tensor` or a list of tensors.  `grad_ys`\n",
      "    is a list of `Tensor`, holding the gradients received by the\n",
      "    `ys`. The list must be the same length as `ys`.\n",
      "    \n",
      "    `gradients()` adds ops to the graph to output the derivatives of `ys` with\n",
      "    respect to `xs`.  It returns a list of `Tensor` of length `len(xs)` where\n",
      "    each tensor is the `sum(dy/dx)` for y in `ys` and for x in `xs`.\n",
      "    \n",
      "    `grad_ys` is a list of tensors of the same length as `ys` that holds\n",
      "    the initial gradients for each y in `ys`.  When `grad_ys` is None,\n",
      "    we fill in a tensor of '1's of the shape of y for each y in `ys`.  A\n",
      "    user can provide their own initial `grad_ys` to compute the\n",
      "    derivatives using a different initial gradient for each y (e.g., if\n",
      "    one wanted to weight the gradient differently for each value in\n",
      "    each y).\n",
      "    \n",
      "    `stop_gradients` is a `Tensor` or a list of tensors to be considered constant\n",
      "    with respect to all `xs`. These tensors will not be backpropagated through,\n",
      "    as though they had been explicitly disconnected using `stop_gradient`.  Among\n",
      "    other things, this allows computation of partial derivatives as opposed to\n",
      "    total derivatives. For example:\n",
      "    \n",
      "    >>> @tf.function\n",
      "    ... def example():\n",
      "    ...   a = tf.constant(0.)\n",
      "    ...   b = 2 * a\n",
      "    ...   return tf.gradients(a + b, [a, b], stop_gradients=[a, b])\n",
      "    >>> example()\n",
      "    [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
      "    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      "    \n",
      "    Here the partial derivatives `g` evaluate to `[1.0, 1.0]`, compared to the\n",
      "    total derivatives `tf.gradients(a + b, [a, b])`, which take into account the\n",
      "    influence of `a` on `b` and evaluate to `[3.0, 1.0]`.  Note that the above is\n",
      "    equivalent to:\n",
      "    \n",
      "    >>> @tf.function\n",
      "    ... def example():\n",
      "    ...   a = tf.stop_gradient(tf.constant(0.))\n",
      "    ...   b = tf.stop_gradient(2 * a)\n",
      "    ...   return tf.gradients(a + b, [a, b])\n",
      "    >>> example()\n",
      "    [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
      "    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      "    \n",
      "    `stop_gradients` provides a way of stopping gradient after the graph has\n",
      "    already been constructed, as compared to `tf.stop_gradient` which is used\n",
      "    during graph construction.  When the two approaches are combined,\n",
      "    backpropagation stops at both `tf.stop_gradient` nodes and nodes in\n",
      "    `stop_gradients`, whichever is encountered first.\n",
      "    \n",
      "    All integer tensors are considered constant with respect to all `xs`, as if\n",
      "    they were included in `stop_gradients`.\n",
      "    \n",
      "    `unconnected_gradients` determines the value returned for each x in xs if it\n",
      "    is unconnected in the graph to ys. By default this is None to safeguard\n",
      "    against errors. Mathematically these gradients are zero which can be requested\n",
      "    using the `'zero'` option. `tf.UnconnectedGradients` provides the\n",
      "    following options and behaviors:\n",
      "    \n",
      "    >>> @tf.function\n",
      "    ... def example(use_zero):\n",
      "    ...   a = tf.ones([1, 2])\n",
      "    ...   b = tf.ones([3, 1])\n",
      "    ...   if use_zero:\n",
      "    ...     return tf.gradients([b], [a], unconnected_gradients='zero')\n",
      "    ...   else:\n",
      "    ...     return tf.gradients([b], [a], unconnected_gradients='none')\n",
      "    >>> example(False)\n",
      "    [None]\n",
      "    >>> example(True)\n",
      "    [<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0., 0.]], ...)>]\n",
      "    \n",
      "    Let us take one practical example which comes during the back propogation\n",
      "    phase. This function is used to evaluate the derivatives of the cost function\n",
      "    with respect to Weights `Ws` and Biases `bs`. Below sample implementation\n",
      "    provides the exaplantion of what it is actually used for :\n",
      "    \n",
      "    >>> @tf.function\n",
      "    ... def example():\n",
      "    ...   Ws = tf.constant(0.)\n",
      "    ...   bs = 2 * Ws\n",
      "    ...   cost = Ws + bs  # This is just an example. Please ignore the formulas.\n",
      "    ...   g = tf.gradients(cost, [Ws, bs])\n",
      "    ...   dCost_dW, dCost_db = g\n",
      "    ...   return dCost_dW, dCost_db\n",
      "    >>> example()\n",
      "    (<tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
      "    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)\n",
      "    \n",
      "    Args:\n",
      "      ys: A `Tensor` or list of tensors to be differentiated.\n",
      "      xs: A `Tensor` or list of tensors to be used for differentiation.\n",
      "      grad_ys: Optional. A `Tensor` or list of tensors the same size as\n",
      "        `ys` and holding the gradients computed for each y in `ys`.\n",
      "      name: Optional name to use for grouping all the gradient ops together.\n",
      "        defaults to 'gradients'.\n",
      "      gate_gradients: If True, add a tuple around the gradients returned\n",
      "        for an operations.  This avoids some race conditions.\n",
      "      aggregation_method: Specifies the method used to combine gradient terms.\n",
      "        Accepted values are constants defined in the class `AggregationMethod`.\n",
      "      stop_gradients: Optional. A `Tensor` or list of tensors not to differentiate\n",
      "        through.\n",
      "      unconnected_gradients: Optional. Specifies the gradient value returned when\n",
      "        the given input tensors are unconnected. Accepted values are constants\n",
      "        defined in the class `tf.UnconnectedGradients` and the default value is\n",
      "        `none`.\n",
      "    \n",
      "    Returns:\n",
      "      A list of `Tensor` of length `len(xs)` where each tensor is the `sum(dy/dx)`\n",
      "      for y in `ys` and for x in `xs`.\n",
      "    \n",
      "    Raises:\n",
      "      LookupError: if one of the operations between `x` and `y` does not\n",
      "        have a registered gradient function.\n",
      "      ValueError: if the arguments are invalid.\n",
      "      RuntimeError: if called in Eager mode.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep  7 16:16:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:00:08.0 Off |                    0 |\n",
      "| N/A   39C    P0             54W /  300W |   31457MiB /  32768MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A         4055161      C   /bin/python3                          31454MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class Timer:  #@save\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "a = tf.Variable(tf.ones(n))\n",
    "b = tf.Variable(tf.ones(n))\n",
    "c = tf.Variable(tf.zeros(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.28958 sec'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer = Timer()\n",
    "for i in range(n):\n",
    "    c[i].assign(a[i] + b[i])\n",
    "f'{timer.stop():.5f} sec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 0.00130 sec'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.start()\n",
    "c = a + b\n",
    "f'{timer.stop(): .5f} sec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../pytools')\n",
    "from d2l import construct_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
